import json
import numpy as np
import torch
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM


class InferlessPythonModel:

    # Implement the Load function here for the model
    def initialize(self):
        self.tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-125m", cache_dir='/var/nfs-mount/new-vol')
        self.model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neo-125m", cache_dir='/var/nfs-mount/new-vol')
        self.device = torch.device("cuda:0")  # Setting device to GPU
        self.model.to(self.device)

        print("This is Initialize Function", flush=True)

    
    # Function to perform inference 
    def infer(self, inputs):
        # inputs is a dictonary where the keys are input names and values are actual input data
        # e.g. in the below code the input name is "prompt"
        prompt = inputs["prompt"]

        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            output = self.model.generate(input_ids, do_sample=True, min_length=20)

        generated_txt = self.tokenizer.decode(output[0], skip_special_tokens=True)
        
        # The output generated by the infer function should be a dictonary where keys are output names and values are actual output data
        # e.g. in the below code the output name is "generated_txt"
        return {"generated_text": generated_txt}

    # perform any cleanup activity here
    def finalize(self,args):
        self.model = None
        self.tokenizer = None
